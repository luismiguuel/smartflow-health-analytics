{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e84d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Treinando RandomForest ---\n",
      "Modelo RandomForest salvo em: ../src/models/RandomForest.pkl\n",
      "\n",
      "--- Treinando XGBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MarceloMelo/Documentos/smartflow-health-analytics/venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:34:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo XGBoost salvo em: ../src/models/XGBoost.pkl\n",
      "\n",
      "--- Treinando LogisticRegression ---\n",
      "Modelo LogisticRegression salvo em: ../src/models/LogisticRegression.pkl\n",
      "\n",
      "--- Treinando SVM ---\n",
      "Modelo SVM salvo em: ../src/models/SVM.pkl\n",
      "\n",
      "Todos os modelos foram treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def carregar_dados_treino(caminho_x, caminho_y):\n",
    "    \"\"\"Carrega os dados que já foram escalonados no pré-processamento.\"\"\"\n",
    "    X_train = pd.read_csv(caminho_x)\n",
    "    y_train = pd.read_csv(caminho_y).values.ravel()\n",
    "    return X_train, y_train\n",
    "\n",
    "def definir_modelos(random_state=42):\n",
    "    \"\"\"\n",
    "    Retorna um dicionário com os modelos configurados.\n",
    "    Como os dados já chegam escalonados, não precisamos de Pipelines aqui.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=10, \n",
    "            random_state=random_state, \n",
    "            class_weight=\"balanced\"\n",
    "        ),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=100, \n",
    "            random_state=random_state, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='mlogloss'\n",
    "        ),\n",
    "        \"LogisticRegression\": LogisticRegression(\n",
    "            class_weight='balanced', \n",
    "            max_iter=1000, \n",
    "            random_state=random_state\n",
    "        ),\n",
    "        \"SVM\": SVC(\n",
    "            kernel='rbf', \n",
    "            probability=True, \n",
    "            class_weight='balanced', \n",
    "            random_state=random_state\n",
    "        )\n",
    "    }\n",
    "\n",
    "def salvar_modelo(modelo, nome_modelo):\n",
    "    \"\"\"Salva o modelo treinado na pasta de modelos.\"\"\"\n",
    "    caminho_saida = f\"../src/models/{nome_modelo}.pkl\"\n",
    "    os.makedirs(os.path.dirname(caminho_saida), exist_ok=True)\n",
    "    joblib.dump(modelo, caminho_saida)\n",
    "    print(f\"Modelo {nome_modelo} salvo em: {caminho_saida}\")\n",
    "\n",
    "def executar_pipeline_treinamento():\n",
    "    \"\"\"Fluxo principal de treinamento multimodelo.\"\"\"\n",
    "    # Caminhos para os dados processados (já escalonados com RobustScaler)\n",
    "    X_TRAIN_PATH = \"../data/processed/X_train.csv\"\n",
    "    Y_TRAIN_PATH = \"../data/processed/y_train.csv\"\n",
    "\n",
    "    # 1. Carga\n",
    "    X_train, y_train = carregar_dados_treino(X_TRAIN_PATH, Y_TRAIN_PATH)\n",
    "    \n",
    "    # 2. Definição\n",
    "    modelos = definir_modelos()\n",
    "\n",
    "    # 3. Treino e Persistência\n",
    "    for nome, modelo in modelos.items():\n",
    "        print(f\"\\n--- Treinando {nome} ---\")\n",
    "        modelo.fit(X_train, y_train)\n",
    "        salvar_modelo(modelo, nome)\n",
    "\n",
    "    print(\"\\nTodos os modelos foram treinados e salvos com sucesso!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    executar_pipeline_treinamento()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983d262",
   "metadata": {},
   "source": [
    "5. MODELAGEM\n",
    "\n",
    "A etapa de modelagem representa o núcleo de inteligência do projeto, onde os dados processados e escalonados são utilizados para treinar algoritmos capazes de realizar a predição do nível de triagem. Esta fase foi desenhada como um experimento comparativo, submetendo o dataset a diferentes paradigmas de aprendizado para identificar qual arquitetura oferece a melhor capacidade preditiva e generalização para o cenário hospitalar.\n",
    "\n",
    "5.1. Seleção e Fundamentação dos Modelos\n",
    "Foram selecionados quatro algoritmos distintos, cada um representando uma abordagem matemática diferente para o problema de classificação:\n",
    "\n",
    "Random Forest Classifier: Um modelo de Ensemble Learning baseado em múltiplas árvores de decisão. Foi escolhido por sua robustez e baixa tendência ao overfitting. Em saúde, sua escolha é justificada pela capacidade de lidar com variáveis de diferentes naturezas (sinais vitais e histórico clínico) e capturar interações não lineares complexas.\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting): Um algoritmo de Gradient Boosting de alta performance. Diferente da Random Forest, ele foca em corrigir sequencialmente os erros das árvores anteriores. Foi escolhido pela sua extrema eficiência em detectar padrões sutis em dados tabulares, o que é vital para separar classes de urgência com limites numéricos muito próximos.\n",
    "\n",
    "Regressão Logística: Implementada como o baseline linear do projeto. Sua escolha fundamenta-se na necessidade de estabelecer uma base de comparação simples e altamente interpretável, permitindo validar se a complexidade dos modelos de árvores ou kernels é realmente necessária para o problema.\n",
    "\n",
    "SVM (Support Vector Machine) com Kernel RBF: Escolhido por sua excelência em encontrar hiperplanos de separação em espaços de alta dimensão. O uso do kernel RBF permite que o modelo crie fronteiras de decisão flexíveis e não lineares, sendo ideal para classificar sinais vitais onde a separação entre \"estável\" e \"crítico\" pode não ser uma linha reta.\n",
    "\n",
    "5.2. Configuração e Justificativa dos Hiperparâmetros\n",
    "Cada modelo foi configurado meticulosamente para equilibrar o desempenho e a estabilidade estatística:\n",
    "\n",
    "Random Forest\n",
    "\n",
    "n_estimators=100 e max_depth=10: Configurou-se um conjunto de 100 árvores com profundidade limitada a 10 níveis. Esta profundidade controlada é uma decisão estratégica para garantir que o modelo aprenda padrões clínicos globais em vez de memorizar casos específicos do treino, promovendo maior robustez.\n",
    "\n",
    "class_weight='balanced': Essencial para lidar com o desbalanceamento, este parâmetro atribui um peso maior aos erros cometidos nas classes minoritárias (Níveis 2 e 3), garantindo que a IA não ignore pacientes graves em favor da acurácia global.\n",
    "\n",
    "XGBoost\n",
    "\n",
    "eval_metric='mlogloss': Utilizou-se a perda logarítmica multiclasse como critério de avaliação interna. Esta métrica penaliza previsões confiantes, mas erradas, forçando o modelo a ser mais preciso nas probabilidades atribuídas a cada nível de triagem.\n",
    "\n",
    "use_label_encoder=False: Aplicado para manter a consistência com o pré-processamento manual de rótulos já realizado, evitando conflitos de codificação entre bibliotecas.\n",
    "\n",
    "Regressão Logística\n",
    "\n",
    "max_iter=1000: O limite de iterações do solver foi expandido para garantir a convergência total do algoritmo. Como o problema envolve múltiplas classes e dados escalonados pelo RobustScaler, um limite maior assegura que o modelo encontre o ponto de erro mínimo sem interrupções prematuras.\n",
    "\n",
    "class_weight='balanced': Aplicado para dar importância proporcional aos casos críticos dentro da função de perda linear.\n",
    "\n",
    "SVM (Support Vector Machine)\n",
    "\n",
    "kernel='rbf' e probability=True: O kernel RBF foi configurado para permitir a separação não linear. A habilitação de probabilidades é fundamental para que o modelo não retorne apenas uma classe rígida, mas também o nível de confiança na predição, o que agrega valor na tomada de decisão médica.\n",
    "\n",
    "class_weight='balanced': Garante que o hiperplano de separação seja posicionado de forma a respeitar a importância dos casos raros (Nível 3).\n",
    "\n",
    "5.3. Padronização e Reprodutibilidade\n",
    "Todos os modelos foram inicializados com random_state=42. Esta configuração garante que todos os processos estocásticos (aleatórios) internos dos algoritmos, como a seleção de atributos na Random Forest ou a inicialização de pesos no SVM, sejam idênticos em cada execução. Isso é fundamental para a integridade científica do relatório, permitindo que os resultados comparativos sejam replicados com exatidão por outros pesquisadores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
